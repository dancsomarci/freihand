{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afcc1d4",
   "metadata": {},
   "source": [
    "# Pose Approximation Refinement\n",
    "\n",
    "Expected Data format:\n",
    "\n",
    "- `x` - images (B, 224, 224, 3)\n",
    "- `y` - ground truth vertices for hand landmarks (B, 21, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46fa96ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  0,  5,  0, 17,  1,  2,  2,  3,  3,  4,  5,  6,  5,  9,  6,  7,\n",
       "          7,  8,  9, 10,  9, 13, 10, 11, 11, 12, 13, 14, 13, 17, 14, 15, 15, 16,\n",
       "         17, 18, 18, 19, 19, 20],\n",
       "        [ 1,  0,  5,  0, 17,  0,  2,  1,  3,  2,  4,  3,  6,  5,  9,  5,  7,  6,\n",
       "          8,  7, 10,  9, 13,  9, 11, 10, 12, 11, 14, 13, 17, 13, 15, 14, 16, 15,\n",
       "         18, 17, 19, 18, 20, 19]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class classproperty(property):\n",
    "    def __get__(self, obj, cls):\n",
    "        return self.fget(cls)\n",
    "\n",
    "class Hand:\n",
    "    # NOTE: MediaPipe 21 point Hand landmark model: https://mediapipe.readthedocs.io/en/latest/solutions/hands.html\n",
    "    _RAW_CONNECTIONS = [\n",
    "        (0,1), (0,5), (0,17), (1,2), (2,3), (3,4),\n",
    "        (5,6), (5,9), (6,7), (7,8), (9,10), (9,13),\n",
    "        (10,11), (11,12), (13,14), (13,17), (14,15),\n",
    "        (15,16), (17,18), (18,19), (19,20)\n",
    "    ]\n",
    "\n",
    "    NUM_POINTS = 21\n",
    "    NUM_FEATURES = 2 # x, y only\n",
    "    \n",
    "    @classproperty\n",
    "    def anatomical(cls):\n",
    "        # Create the edge list and make it undirected by adding reverse connections (same for all graphs)\n",
    "        temp = []\n",
    "        for a, b in cls._RAW_CONNECTIONS:\n",
    "            temp.append([a, b])\n",
    "            temp.append([b, a])  # Add the reverse connection\n",
    "        return temp\n",
    "    \n",
    "    @classproperty\n",
    "    def inverse_anatomical(cls):\n",
    "        inverse_graph_connections = []\n",
    "        for i in range(21):\n",
    "            for j in range(21):\n",
    "                conn = (i, j)\n",
    "                if conn not in cls._RAW_CONNECTIONS and i != j:\n",
    "                    inverse_graph_connections.append(conn)\n",
    "        return inverse_graph_connections\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_edge_index(connection_choice):\n",
    "        return torch.tensor(connection_choice, dtype=torch.long).t().contiguous()\n",
    "\n",
    "Hand.compute_edge_index(Hand.anatomical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ba8c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import lmdb\n",
    "\n",
    "class PoseLMDBDataset(Dataset):\n",
    "    def __init__(self, lmdb_path):\n",
    "        self.env = lmdb.open(lmdb_path,\n",
    "            readonly=True, lock=False,\n",
    "            readahead=False, meminit=False)\n",
    "        self.txn = self.env.begin(write=False)\n",
    "        self.length = self.txn.stat()['entries']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = f\"{idx:08d}\".encode('ascii')\n",
    "        byteflow = self.txn.get(key)\n",
    "        sample = pickle.loads(byteflow)\n",
    "        img = sample[\"image\"]\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        img = torch.from_numpy(img).float() / 255.0\n",
    "        return img, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c50e2d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 224, 224, 3]) torch.Size([256, 21, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BME\\projects\\frei-hands\\freihand\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "dataset = PoseLMDBDataset(\"training_data.lmdb\")\n",
    "\n",
    "# Split lengths for train (80%), test (10%), valid (10%)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "240c5b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m random_indices = random.sample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)), \u001b[32m3\u001b[39m)\n\u001b[32m     51\u001b[39m random_indices = [\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mvisualize_graphs_side_by_side\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mvisualize_graphs_side_by_side\u001b[39m\u001b[34m(dataset, indices)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ax, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axs, indices):\n\u001b[32m     13\u001b[39m     data = dataset[idx]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     mp_lms = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmediapipe_lms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.numpy()\n\u001b[32m     15\u001b[39m     real_lms = data[\u001b[33m\"\u001b[39m\u001b[33mreal_lms\u001b[39m\u001b[33m\"\u001b[39m].numpy()\n\u001b[32m     16\u001b[39m     img = data[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m].numpy() \u001b[38;5;66;03m# shape (H, W, 3), normalized 0-1\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not str"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graphs_side_by_side(dataset, indices):\n",
    "    num_graphs = len(indices)\n",
    "    fig, axs = plt.subplots(1, num_graphs, figsize=(5 * num_graphs, 5))\n",
    "    axs = [axs] if num_graphs == 1 else axs\n",
    "\n",
    "    fig.suptitle(\"Comparsion of Graphs\", fontsize=20)\n",
    "\n",
    "    for ax, idx in zip(axs, indices):\n",
    "        data = dataset[idx]\n",
    "        mp_lms = data[\"mediapipe_lms\"].numpy()\n",
    "        real_lms = data[\"real_lms\"].numpy()\n",
    "        img = data[\"image\"].numpy() # shape (H, W, 3), normalized 0-1\n",
    "\n",
    "        # Background image (fit to 0-1 coordinate space)\n",
    "        ax.imshow(img, extent=[0, 1, 0, 1], origin='upper', aspect='auto')\n",
    "\n",
    "        # Graph edges\n",
    "        edge_index = Hand.compute_edge_index(Hand.anatomical).numpy()\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edge_index.T)\n",
    "\n",
    "        # Node features\n",
    "        x1, y1 = mp_lms[:, 0], mp_lms[:, 1]\n",
    "        y1 = 1.0 - y1\n",
    "        pos_x = {i: (x1[i], y1[i]) for i in range(len(x1))}\n",
    "        nx.draw_networkx_nodes(G, pos_x, node_size=50, node_color='blue', ax=ax)\n",
    "        nx.draw_networkx_edges(G, pos_x, width=1.5, alpha=0.7, ax=ax)\n",
    "\n",
    "        x2, y2 = real_lms[:, 0], real_lms[:, 1]\n",
    "        y2 = 1.0 - y2\n",
    "        pos_y = {i: (x2[i], y2[i]) for i in range(len(x2))}\n",
    "        nx.draw_networkx_nodes(G, pos_y, node_size=50, node_color='red', ax=ax)\n",
    "\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.invert_yaxis()  # optional, depends on how you want y-axis\n",
    "        ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True, labelsize=12)\n",
    "        ax.set_xlabel(\"X-axis\", fontsize=14)\n",
    "        ax.set_ylabel(\"Y-axis\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "random_indices = random.sample(range(len(dataset)), 3)\n",
    "random_indices = [0,1,2]\n",
    "visualize_graphs_side_by_side(dataset, indices=random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2e663ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MobileGNNConv(nn.Module):\n",
    "    \"\"\"Custom implementation of GCNConv in Pytorch for Static ASL Fingerspelling sign classification.\n",
    "\n",
    "    The implementation optimizes the general GCN by making the adjacency matrix, denoted 'A', fixed in size for classic batching.\n",
    "    It also allows for optimizing the values of the said matrix via gradient descent and dynamically freezing the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(MobileGNNConv, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "\n",
    "        A = torch.zeros((Hand.NUM_POINTS, Hand.NUM_POINTS))\n",
    "        for x, y in Hand.anatomical:\n",
    "          A[x, y] = 1\n",
    "\n",
    "        A_hat = A + torch.eye(A.size(0)) # self loops\n",
    "        D = torch.diag(A_hat.sum(dim=1))\n",
    "        D_neg_sqrt = torch.linalg.inv(torch.sqrt(D))\n",
    "        A_norm = D_neg_sqrt @ A_hat @ D_neg_sqrt\n",
    "        self.A_norm = nn.Parameter(A_norm, requires_grad=False) # Constant by default\n",
    "\n",
    "    def set_adjancency(self, trainable: bool):\n",
    "        self.A_norm.requires_grad = trainable\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        # ReLU(D^-1/2 * A_hat * D^-1/2 * X * W)\n",
    "        return F.relu(self.lin(self.A_norm @ X))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.input_dim) + ' -> ' + str(self.output_dim) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18adfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePoseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.extract_features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1),  # (B, 16, 112, 112)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # (B, 32, 56, 56)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # (B, 64, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # (B, 128, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)  # (B, 128, 1, 1)\n",
    "        )\n",
    "        # Regressor to keypoints\n",
    "        self.fc = nn.Linear(128, Hand.NUM_POINTS * Hand.NUM_FEATURES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, 224, 224)\n",
    "        B = x.size(0)\n",
    "        features = self.extract_features(x) # (B, 128, 1, 1)\n",
    "        features = features.view(B, -1)     # (B, 128)\n",
    "        out = self.fc(features)             # (B, 42)\n",
    "        return out.view(\n",
    "            B, Hand.NUM_POINTS, Hand.NUM_FEATURES\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8e4821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "class ResnetPoseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    " \n",
    "        # ResNet18 backbone\n",
    "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1, progress=False)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # remove FC\n",
    "        # Freeze all parameters of ResNet\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(512, Hand.NUM_POINTS * Hand.NUM_FEATURES)\n",
    " \n",
    "    def forward(self, images: torch.Tensor):\n",
    "        \"\"\"\n",
    "        images: (B, H, W, 3)\n",
    "        returns: (B, 21, 2)\n",
    "        \"\"\"\n",
    "        B = images.size(0)\n",
    "        images = images.permute(0, 3, 1, 2) # (B, 3, H, W)\n",
    "        x = self.resnet(images)  # (B, 512, 1, 1)\n",
    "        x = self.flatten(x)      # (B, 512)\n",
    "        x = self.fc(x)           # (B, 42)\n",
    "        x = x.view(B, Hand.NUM_POINTS, Hand.NUM_FEATURES)  # (B, 21, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1a96b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class HandGCN(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, gcn_latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.gcn1 = MobileGNNConv(Hand.NUM_FEATURES, gcn_latent_dim)\n",
    "        self.gcn2 = MobileGNNConv(gcn_latent_dim, Hand.NUM_FEATURES)\n",
    "\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        \"\"\"\n",
    "        images: (B, H, W, 3)\n",
    "        returns: (B, 21, 2)\n",
    "        \"\"\"\n",
    "        B = images.size(0)\n",
    "        images = images.permute(0, 3, 1, 2) # (B, 3, H, W)\n",
    "        x = self.backbone(images)  # (B, 21, 2)\n",
    "\n",
    "        # Refine coordinates using GCN\n",
    "        x = self.gcn1(x)\n",
    "        x = self.gcn2(x)\n",
    "        return x\n",
    "    \n",
    "model = HandGCN(backbone=SimplePoseModel(), gcn_latent_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8df7416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 103996\n",
      "Non-Trainable parameters: 882\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total, trainable = count_parameters(model)\n",
    "print(f\"Total parameters: {total}\")\n",
    "print(f\"Non-Trainable parameters: {total- trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fe845de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train | MSE: 0.2127, MAE: 0.4368, PCK: 0.0000\n",
      "         Val   | MSE: 0.1617, MAE: 0.3583, PCK: 0.0000\n",
      "\n",
      "[Epoch 2] Train | MSE: 0.1417, MAE: 0.3194, PCK: 0.0000\n",
      "         Val   | MSE: 0.1326, MAE: 0.2982, PCK: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def pck(preds, targets, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Computes PCK (Percentage of Correct Keypoints within a tolerance)\n",
    "    \"\"\"\n",
    "    dists = torch.norm(preds - targets, dim=-1)\n",
    "    correct = (dists < threshold).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "def train_pose_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, epochs=2, lr=1e-3, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss, train_mae, train_pck = 0.0, 0.0, 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            imgs, _mp_lms, real_lms = batch[\"image\"], batch[\"mediapipe_lms\"], batch[\"real_lms\"]\n",
    "            inputs, targets = imgs.to(device), real_lms.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = criterion(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_mae += torch.abs(preds - targets).mean().item()\n",
    "            train_pck += pck(preds, targets)\n",
    "\n",
    "        n_train = len(train_loader)\n",
    "        print(f\"[Epoch {epoch}] Train | \"\n",
    "              f\"MSE: {train_loss/n_train:.4f}, \"\n",
    "              f\"MAE: {train_mae/n_train:.4f}, \"\n",
    "              f\"PCK: {train_pck/n_train:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss, val_mae, val_pck = 0.0, 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, _mp_lms, real_lms = batch[\"image\"], batch[\"mediapipe_lms\"], batch[\"real_lms\"]\n",
    "                inputs, targets = imgs.to(device), real_lms.to(device)\n",
    "                preds = model(inputs)\n",
    "\n",
    "                val_loss += criterion(preds, targets).item()\n",
    "                val_mae += torch.abs(preds - targets).mean().item()\n",
    "                val_pck += pck(preds, targets)\n",
    "\n",
    "        n_val = len(val_loader)\n",
    "        print(f\"         Val   | \"\n",
    "              f\"MSE: {val_loss/n_val:.4f}, \"\n",
    "              f\"MAE: {val_mae/n_val:.4f}, \"\n",
    "              f\"PCK: {val_pck/n_val:.4f}\\n\")\n",
    "\n",
    "train_pose_model(model, train_loader, valid_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fc9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_graphs = len(indices)\n",
    "data = valid_dataset\n",
    "fig, axs = plt.subplots(1, num_graphs, figsize=(5 * num_graphs, 5))\n",
    "axs = [axs] if num_graphs == 1 else axs\n",
    "\n",
    "fig.suptitle(\"Comparsion of Graphs\", fontsize=20)\n",
    "\n",
    "for ax, idx in zip(axs, indices):\n",
    "    data = dataset[idx]\n",
    "    mp_lms = data[\"mediapipe_lms\"].numpy()\n",
    "    real_lms = data[\"real_lms\"].numpy()\n",
    "    img = data[\"image\"].numpy() # shape (H, W, 3), normalized 0-1\n",
    "\n",
    "    # Background image (fit to 0-1 coordinate space)\n",
    "    ax.imshow(img, extent=[0, 1, 0, 1], origin='upper', aspect='auto')\n",
    "\n",
    "    # Graph edges\n",
    "    edge_index = Hand.compute_edge_index(Hand.anatomical).numpy()\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edge_index.T)\n",
    "\n",
    "    # Node features\n",
    "    x1, y1 = mp_lms[:, 0], mp_lms[:, 1]\n",
    "    y1 = 1.0 - y1\n",
    "    pos_x = {i: (x1[i], y1[i]) for i in range(len(x1))}\n",
    "    nx.draw_networkx_nodes(G, pos_x, node_size=50, node_color='blue', ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos_x, width=1.5, alpha=0.7, ax=ax)\n",
    "\n",
    "    x2, y2 = real_lms[:, 0], real_lms[:, 1]\n",
    "    y2 = 1.0 - y2\n",
    "    pos_y = {i: (x2[i], y2[i]) for i in range(len(x2))}\n",
    "    nx.draw_networkx_nodes(G, pos_y, node_size=50, node_color='red', ax=ax)\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.invert_yaxis()  # optional, depends on how you want y-axis\n",
    "    ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True, labelsize=12)\n",
    "    ax.set_xlabel(\"X-axis\", fontsize=14)\n",
    "    ax.set_ylabel(\"Y-axis\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example usage\n",
    "random_indices = random.sample(range(len(dataset)), 3)\n",
    "random_indices = [0,1,2]\n",
    "visualize_graphs_side_by_side(dataset, indices=random_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freihand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
